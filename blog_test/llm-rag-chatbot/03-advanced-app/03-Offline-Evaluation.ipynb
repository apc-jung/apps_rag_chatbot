{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e79c7882-7b01-479d-a12f-adc7885749cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Turn the Review App logs into an Evaluation Set\n",
    "\n",
    "The Review application captures your user feedbacks.\n",
    "\n",
    "This feedback is saved under 2 tables within your schema.\n",
    "\n",
    "In this notebook, we will show you how to extract the logs from the Review App into an Evaluation Set.  It is important to review each row and ensure the data quality is high e.g., the question is logical and the response makes sense.\n",
    "\n",
    "1. Requests with a üëç :\n",
    "    - `request`: As entered by the user\n",
    "    - `expected_response`: If the user edited the response, that is used, otherwise, the model's generated response.\n",
    "2. Requests with a üëé :\n",
    "    - `request`: As entered by the user\n",
    "    - `expected_response`: If the user edited the response, that is used, otherwise, null.\n",
    "3. Requests without any feedback\n",
    "    - `request`: As entered by the user\n",
    "\n",
    "Across all types of requests, if the user üëç a chunk from the `retrieved_context`, the `doc_uri` of that chunk is included in `expected_retrieved_context` for the question.\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection or disable tracker during installation. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=data-science&org_id=341332174749405&notebook=%2F03-advanced-app%2F03-Offline-Evaluation&demo_name=llm-rag-chatbot&event=VIEW&path=%2F_dbdemos%2Fdata-science%2Fllm-rag-chatbot%2F03-advanced-app%2F03-Offline-Evaluation&version=1\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbe98a03-c598-4bac-ba58-385789edc835",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --quiet -U databricks-sdk==0.49.0 databricks-agents\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "829445c1-9335-4bcc-a43d-95dc7015d8ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 1.1/ Extracting the logs \n",
    "\n",
    "\n",
    "*Note: for now, this part requires a few SQL queries that we provide in this notebook to properly format the review app into training dataset.*\n",
    "\n",
    "*We'll update this notebook soon with an simpler version - stay tuned!*\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection or disable tracker during installation. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=data-science&org_id=341332174749405&notebook=%2F03-advanced-app%2F03-Offline-Evaluation&demo_name=llm-rag-chatbot&event=VIEW&path=%2F_dbdemos%2Fdata-science%2Fllm-rag-chatbot%2F03-advanced-app%2F03-Offline-Evaluation&version=1\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f78dd2d-e953-4d62-9c45-ef3b59d4390a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ../_resources/00-init-advanced $reset_all_data=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "184337da-3743-4a81-9ea9-93ded7634fe2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks import agents\n",
    "MODEL_NAME = \"rag_demo_advanced\"\n",
    "MODEL_NAME_FQN = f\"{catalog}.{db}.{MODEL_NAME}\"\n",
    "browser_url = mlflow.utils.databricks_utils.get_browser_hostname()\n",
    "\n",
    "# # Get the name of the Inference Tables where logs are stored\n",
    "active_deployments = agents.list_deployments()\n",
    "active_deployment = next((item for item in active_deployments if item.model_name == MODEL_NAME_FQN), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9330261e-0476-4eb1-8bc8-fd511d44e622",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "w = WorkspaceClient()\n",
    "print(active_deployment)\n",
    "endpoint = w.serving_endpoints.get(active_deployment.endpoint_name)\n",
    "\n",
    "try:\n",
    "    endpoint_config = endpoint.config.auto_capture_config\n",
    "except AttributeError as e:\n",
    "    endpoint_config = endpoint.pending_config.auto_capture_config\n",
    "\n",
    "inference_table_name = endpoint_config.state.payload_table.name\n",
    "inference_table_catalog = endpoint_config.catalog_name\n",
    "inference_table_schema = endpoint_config.schema_name\n",
    "\n",
    "# Cleanly formatted tables\n",
    "assessment_table = f\"{inference_table_catalog}.{inference_table_schema}.`{inference_table_name}_assessment_logs`\"\n",
    "request_table = f\"{inference_table_catalog}.{inference_table_schema}.`{inference_table_name}_request_logs`\"\n",
    "\n",
    "# Note: you might have to wait a bit for the tables to be ready\n",
    "print(f\"Request logs: {request_table}\")\n",
    "requests_df = spark.table(request_table)\n",
    "print(f\"Assessment logs: {assessment_table}\")\n",
    "#Temporary helper to extract the table - see _resources/00-init-advanced \n",
    "assessment_df = deduplicate_assessments_table(assessment_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1202a48b-d763-4ce4-85d2-ffc8d0fc7339",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "requests_with_feedback_df = requests_df.join(assessment_df, requests_df.databricks_request_id == assessment_df.request_id, \"left\")\n",
    "display(requests_with_feedback_df.select(\"request_raw\", \"trace\", \"source\", \"text_assessment\", \"retrieval_assessments\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "def78ed8-adf9-4839-8454-c6b9ecd471c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "requests_with_feedback_df.createOrReplaceTempView('latest_assessments')\n",
    "eval_dataset = spark.sql(f\"\"\"\n",
    "-- Thumbs up.  Use the model's generated response as the expected_response\n",
    "select\n",
    "  a.request_id,\n",
    "  r.request,\n",
    "  r.response as expected_response,\n",
    "  'thumbs_up' as type,\n",
    "  a.source.id as user_id\n",
    "from\n",
    "  latest_assessments as a\n",
    "  join {request_table} as r on a.request_id = r.databricks_request_id\n",
    "where\n",
    "  a.text_assessment.ratings [\"answer_correct\"].value == \"positive\"\n",
    "union all\n",
    "  --Thumbs down.  If edited, use that as the expected_response.\n",
    "select\n",
    "  a.request_id,\n",
    "  r.request,\n",
    "  IF(\n",
    "    a.text_assessment.suggested_output != \"\",\n",
    "    a.text_assessment.suggested_output,\n",
    "    NULL\n",
    "  ) as expected_response,\n",
    "  'thumbs_down' as type,\n",
    "  a.source.id as user_id\n",
    "from\n",
    "  latest_assessments as a\n",
    "  join {request_table} as r on a.request_id = r.databricks_request_id\n",
    "where\n",
    "  a.text_assessment.ratings [\"answer_correct\"].value = \"negative\"\n",
    "union all\n",
    "  -- No feedback.  Include the request, but no expected_response\n",
    "select\n",
    "  a.request_id,\n",
    "  r.request,\n",
    "  IF(\n",
    "    a.text_assessment.suggested_output != \"\",\n",
    "    a.text_assessment.suggested_output,\n",
    "    NULL\n",
    "  ) as expected_response,\n",
    "  'no_feedback_provided' as type,\n",
    "  a.source.id as user_id\n",
    "from\n",
    "  latest_assessments as a\n",
    "  join {request_table} as r on a.request_id = r.databricks_request_id\n",
    "where\n",
    "  a.text_assessment.ratings [\"answer_correct\"].value != \"negative\"\n",
    "  and a.text_assessment.ratings [\"answer_correct\"].value != \"positive\"\n",
    "  \"\"\")\n",
    "display(eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2304851-229a-4bae-b527-5f6dabbb73d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1.2/ Our eval dataset is now ready! \n",
    "\n",
    "The review app makes it easy to build & create your evaluation dataset. \n",
    "\n",
    "*Note: the eval app logs may take some time to be available to you. If the dataset is empty, wait a bit.*\n",
    "\n",
    "To simplify the demo and make sure you don't have to craft your own eval dataset, we saved a ready-to-use eval dataset already pre-generated for you. We'll use this one for the demo instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "273a5ff2-49bf-4a22-a550-cca49eddd4b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "eval_dataset = spark.table(\"eval_set_databricks_documentation\").limit(10)\n",
    "display(eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09b02754-df2f-48e1-8636-20a17837e566",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load the correct Python environment for the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4fe24938-2d91-4a92-8379-b254b4e14680",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Retrieve the model we want to eval\n",
    "model = get_latest_model(MODEL_NAME_FQN)\n",
    "pip_requirements = mlflow.pyfunc.get_model_dependencies(f\"runs:/{model.run_id}/chain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42321c57-55e7-4bf0-a0fd-97ac93d9eba8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "install the model requirements to avoid version conflict"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -r $pip_requirements\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c61ec299-876b-46ae-8951-4a2cb8bf7eac",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "just reload our catalog/schema name for the demo"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ../_resources/00-init-advanced $reset_all_data=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b220fcf-980c-4bb5-b282-50dcb727a5e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Run our evaluation from the dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d636f20-f0f2-42f4-911b-72b8f80dbcca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"rag_demo_advanced\"\n",
    "MODEL_NAME_FQN = f\"{catalog}.{db}.{MODEL_NAME}\"\n",
    "model = get_latest_model(MODEL_NAME_FQN)\n",
    "with mlflow.start_run(run_name=\"eval_dataset_advanced\"):\n",
    "    # Evaluate the logged model\n",
    "    eval_results = mlflow.evaluate(\n",
    "        data=spark.table(\"eval_set_databricks_documentation\"),\n",
    "        model=f'runs:/{model.run_id}/chain',\n",
    "        model_type=\"databricks-agent\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f83b0051-ec5d-4018-bd9e-82c5c3cb19c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You can open MLFlow and review the eval metrics, and also compare it to previous eval runs!\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/mlflow-eval.gif?raw=true\" width=\"1200px\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf1d8cb6-9536-4371-92b3-5b567180204f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### This is looking good, let's tag our model as production ready\n",
    "\n",
    "After reviewing the model correctness and potentially comparing its behavior to your other previous version, we can flag our model as ready to be deployed.\n",
    "\n",
    "*Note: Evaluation can be automated and part of a MLOps step: once you deploy a new Chatbot version with a new prompt, run the evaluation job and benchmark your model behavior vs the previous version.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af19d6e5-3b69-4546-a6a3-8ebc27ff7c4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "client = MlflowClient()\n",
    "client.set_registered_model_alias(name=MODEL_NAME_FQN, alias=\"prod\", version=model.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64cc7130-f00f-42d6-882c-e04a0a3883b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In a production setup, we would deploy another PROD model endpoint serving using this mode. To keep this demo simple, we will keep our previous endpoint for our next online evaluation step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48d9d1ec-51fb-46c6-9a79-1630e423ce70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Next step: track production online inferences with Databricks AI Gateway\n",
    "\n",
    "Mosaic AI Agent Evaluation makes it easy to evaluate your LLM Models, leveraging custom metrics.\n",
    "\n",
    "Evaluating your chatbot is key to measure your future version impact, and your Data Intelligence Platform makes it easy leveraging automated Workflow for your MLOps pipelines.\n",
    "\n",
    "Let's now review how to track our production model endpoint, tracking our users question through Databricks AI Gateway: [open 04-Online-Evaluation]($./04-Online-Evaluation)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "03-Offline-Evaluation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
