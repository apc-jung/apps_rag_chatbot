{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "04223f8d-9989-4763-bb5d-0d9ba385d0ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# 1/ Ingesting and preparing PDF for LLM and Self Managed Vector Search Embeddings\n",
    "\n",
    "## In this example, we will focus on ingesting pdf documents as source for our retrieval process. \n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/rag-pdf-self-managed-0.png?raw=true\" style=\"float: right; width: 600px; margin-left: 10px\">\n",
    "\n",
    "\n",
    "For this example, we will add Databricks ebook PDFs from [Databricks resources page](https://www.databricks.com/resources) to our knowledge database.\n",
    "\n",
    "**Note: This demo is advanced content, we strongly recommend going over the simple version first to learn the basics.**\n",
    "\n",
    "Here are all the detailed steps:\n",
    "\n",
    "- Use autoloader to load the binary PDFs into our first table. \n",
    "- Use the `unstructured` library  to parse the text content of the PDFs.\n",
    "- Use `llama_index` or `Langchain` to split the texts into chuncks.\n",
    "- Compute embeddings for the chunks.\n",
    "- Save our text chunks + embeddings in a Delta Lake table, ready for Vector Search indexing.\n",
    "\n",
    "\n",
    "Lakehouse AI not only provides state of the art solutions to accelerate your AI and LLM projects, but also to accelerate data ingestion and preparation at scale, including unstructured data like PDFs.\n",
    "\n",
    "<div style=\"background-color: #d4f8d4; border-radius: 15px; padding: 20px; text-align: center;\">\n",
    "        Note: Looking for a full, production-grade guide? Make sure you checkout <a target=\"_blank\" href=\"https://ai-cookbook.io\">Databricks ai-cookbook.ai </a>!\n",
    "    </div>\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection or disable tracker during installation. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=data-science&org_id=341332174749405&notebook=%2F03-advanced-app%2F01-PDF-Advanced-Data-Preparation&demo_name=llm-rag-chatbot&event=VIEW&path=%2F_dbdemos%2Fdata-science%2Fllm-rag-chatbot%2F03-advanced-app%2F01-PDF-Advanced-Data-Preparation&version=1\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21ce4437-f44e-4f78-b710-4d183add0197",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install required external libraries "
    }
   },
   "outputs": [],
   "source": [
    "%pip install --quiet -U transformers==4.49.0 pypdf==4.1.0 langchain-text-splitters==0.2.0 databricks-vectorsearch==0.55 mlflow[databricks] tiktoken==0.7.0 torch==2.3.0 llama-index==0.10.43 markdownify==0.14.1\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86bd12e5-fae1-4a61-9603-8eaef0e1129c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ../_resources/00-init-advanced $reset_all_data=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "a047451a-1b92-474b-8b9e-441f52520296",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Ingesting Databricks ebook PDFs and extracting their pages\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/rag-pdf-self-managed-1.png?raw=true\" style=\"float: right\" width=\"500px\">\n",
    "\n",
    "First, let's ingest our PDFs as a Delta Lake table with path urls and content in binary format. \n",
    "\n",
    "We'll use [Databricks Autoloader](https://docs.databricks.com/en/ingestion/auto-loader/index.html) to incrementally ingeset new files, making it easy to incrementally consume billions of files from the data lake in various data formats. Autoloader easily ingests our unstructured PDF data in binary format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f1c37ee-8288-46b2-9863-447e83549699",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE VOLUME IF NOT EXISTS volume_databricks_documentation;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "233e04ab-eae9-41c3-b948-65c218edb005",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Our pdf or docx files are available in our Volume (or DBFS)"
    }
   },
   "outputs": [],
   "source": [
    "# List our raw PDF docs\n",
    "volume_folder =  f\"/Volumes/{catalog}/{db}/volume_databricks_documentation\"\n",
    "# Let's upload some pdf files to our volume as example. Change this with your own PDFs / docs.\n",
    "upload_pdfs_to_volume(volume_folder+\"/databricks-pdf\")\n",
    "\n",
    "display(dbutils.fs.ls(volume_folder+\"/databricks-pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a20001e-1eea-4a37-a333-2039c2b772b9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Ingesting PDF files as binary format using Databricks cloudFiles (Autoloader)"
    }
   },
   "outputs": [],
   "source": [
    "df = (spark.readStream\n",
    "        .format('cloudFiles')\n",
    "        .option('cloudFiles.format', 'BINARYFILE')\n",
    "        .option(\"pathGlobFilter\", \"*.pdf\")\n",
    "        .load('dbfs:'+volume_folder+\"/databricks-pdf\"))\n",
    "\n",
    "# Write the data as a Delta table\n",
    "(df.writeStream\n",
    "  .trigger(availableNow=True)\n",
    "  .option(\"checkpointLocation\", f'dbfs:{volume_folder}/checkpoints/raw_docs')\n",
    "  .table('pdf_raw').awaitTermination())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67f33256-08d3-4c13-afcb-82ed26ac9812",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql SELECT * FROM pdf_raw LIMIT 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "3ca5b6a5-86fb-4cf7-a38d-1780d2862f85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/rag-pdf-self-managed-2.png?raw=true\" style=\"float: right\" width=\"500px\">\n",
    "\n",
    "## Extracting our PDF content as text chunks\n",
    "\n",
    "We need to convert the PDF documents bytes to text, and extract chunks from their content.\n",
    "\n",
    "This part can be tricky as PDFs are hard to work with and can be saved as images, for which we'll need an OCR to extract the text.\n",
    "\n",
    "Using the `Unstructured` library within a Spark UDF makes it easy to extract text. \n",
    "\n",
    "*Note: Your cluster will need a few extra libraries that you would typically install with a cluster init script.*\n",
    "\n",
    "<br style=\"clear: both\">\n",
    "\n",
    "### Splitting our big documentation page in smaller chunks\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/chunk-window-size.png?raw=true\" style=\"float: right\" width=\"700px\">\n",
    "\n",
    "In this demo, some PDFs are very large, with a lot of text.\n",
    "\n",
    "We'll extract the content and then use llama_index `SentenceSplitter`, and ensure that each chunk isn't bigger than 500 tokens. \n",
    "\n",
    "\n",
    "The chunk size and chunk overlap depend on the use case and the PDF files. \n",
    "\n",
    "Remember that your prompt+answer should stay below your model max window size (4096 for llama2). \n",
    "\n",
    "For more details, review the previous [../01-Data-Preparation](01-Data-Preparation) notebook. \n",
    "\n",
    "<br/>\n",
    "<br style=\"clear: both\">\n",
    "<div style=\"background-color: #def2ff; padding: 15px;  border-radius: 30px; \">\n",
    "  <strong>Information</strong><br/>\n",
    "  Remember that the following steps are specific to your dataset. This is a critical part to building a successful RAG assistant.\n",
    "  <br/> Always take time to review the chunks created and ensure they make sense and contain relevant information.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "737392ce-2e32-4238-acee-d5dfe6d7d011",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "To extract our PDF,  we'll need to setup libraries in our nodes"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from pypdf import PdfReader\n",
    "\n",
    "def parse_bytes_pypdf(raw_doc_contents_bytes: bytes):\n",
    "    #Note: in production setting you might want to try/catch errors here to handle incorrect pdf/files\n",
    "    pdf = io.BytesIO(raw_doc_contents_bytes)\n",
    "    reader = PdfReader(pdf)\n",
    "    parsed_content = [page_content.extract_text() for page_content in reader.pages]\n",
    "    return \"\\n\".join(parsed_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3eb2b17b-1638-47d9-93df-9bcabdc15626",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's start by extracting text from our PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "318cb407-0ab7-4685-ae9c-22586adc2d3c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Trying our text extraction function with a single pdf file"
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "with requests.get('https://dbdemos-dataset.s3.amazonaws.com/llm/databricks-pdf-documentation/Databricks-Customer-360-ebook-Final.pdf') as pdf:\n",
    "  doc = parse_bytes_pypdf(pdf.content)  \n",
    "  print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f74da653-2d94-420f-a462-0bec5867cc31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This looks great. We'll now wrap it with a text_splitter to avoid having too big pages, and create a Pandas UDF function to easily scale that across multiple nodes.\n",
    "\n",
    "*Note that our pdf text isn't clean. To make it nicer, we could use a few extra LLM-based pre-processing steps, asking to remove unrelevant content like the list of chapters and to only keep the core text.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e53ff749-8115-4cc8-93a6-f55f1827233c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import Document, set_global_tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "from typing import Iterator\n",
    "\n",
    "# Reduce the arrow batch size as our PDF can be big in memory (classic compute only)\n",
    "# spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", 10)\n",
    "\n",
    "@pandas_udf(\"array<string>\")\n",
    "def read_as_chunk(batch_iter: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    #set llama2 as tokenizer to match our model size (will stay below gte 1024 limit)\n",
    "    set_global_tokenizer(\n",
    "      AutoTokenizer.from_pretrained(\"hf-internal-testing/llama-tokenizer\", cache_dir=\"/tmp/hf_cache\")\n",
    "    )\n",
    "    #Sentence splitter from llama_index to split on sentences\n",
    "    splitter = SentenceSplitter(chunk_size=500, chunk_overlap=10)\n",
    "    def extract_and_split(b):\n",
    "      try:\n",
    "        txt = parse_bytes_pypdf(b)\n",
    "      except Exception as e:\n",
    "        txt = f'__PDF_PARSING_ERROR__ file: {e}'\n",
    "        print(txt)\n",
    "      if txt is None:\n",
    "        return []\n",
    "      nodes = splitter.get_nodes_from_documents([Document(text=txt)])\n",
    "      return [n.text for n in nodes]\n",
    "\n",
    "    for x in batch_iter:\n",
    "        yield x.apply(extract_and_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "7704dd06-9ce4-4e1f-8b52-b3b2a36a7333",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## What's required for our Vector Search Index\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/databricks-vector-search-type.png?raw=true\" style=\"float: right\" width=\"800px\">\n",
    "\n",
    "Databricks provide multiple types of vector search indexes:\n",
    "\n",
    "- **Managed embeddings**: you provide a text column and endpoint name and Databricks synchronizes the index with your Delta table \n",
    "- **Self Managed embeddings**: you compute the embeddings and save them as a field of your Delta Table, Databricks will then synchronize the index\n",
    "- **Direct index**: when you want to use and update the index without having a Delta Table.\n",
    "\n",
    "In this demo, we will show you how to setup a **Self-managed Embeddings** index. \n",
    "\n",
    "To do so, we will have to first compute the embeddings of our chunks and save them as a Delta Lake table field as `array&ltfloat&gt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "0fa66493-1c95-4edd-8b5b-7d2fb4cf1ffa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Introducing Databricks GTE Embeddings Foundation Model endpoints\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/rag-pdf-self-managed-4.png?raw=true\" style=\"float: right; width: 600px; margin-left: 10px\">\n",
    "\n",
    "Foundation Models are provided by Databricks, and can be used out-of-the-box.\n",
    "\n",
    "Databricks supports several endpoint types to compute embeddings or evaluate a model:\n",
    "- DBRX Instruct, a **foundation model endpoint**, or another model served by databricks (ex: llama2-70B, MPT...)\n",
    "- An **external endpoint**, acting as a gateway to an external model (ex: Azure OpenAI)\n",
    "- A **custom**, fined-tuned model hosted on Databricks model service\n",
    "\n",
    "Open the [Model Serving Endpoint page](/ml/endpoints) to explore and try the foundation models.\n",
    "\n",
    "For this demo, we will use the foundation model `GTE` (embeddings) and `DBRX` (chat). <br/><br/>\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/databricks-foundation-models.png?raw=true\" width=\"600px\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9bbbda3d-3364-4d50-b233-815a07ef23c1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Using Databricks Foundation model GTE as embedding endpoint"
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.deployments import get_deploy_client\n",
    "\n",
    "# gte-large-en Foundation models are available using the /serving-endpoints/databricks-gtegte-large-en/invocations api. \n",
    "deploy_client = get_deploy_client(\"databricks\")\n",
    "\n",
    "## NOTE: if you change your embedding model here, make sure you change it in the query step too\n",
    "embeddings = deploy_client.predict(endpoint=\"databricks-gte-large-en\", inputs={\"input\": [\"What is Apache Spark?\"]})\n",
    "pprint(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb651924-74a0-4568-aad3-d8191c292b27",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create the final databricks_pdf_documentation table containing chunks and embeddings"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--Note that we need to enable Change Data Feed on the table to create the index\n",
    "CREATE TABLE IF NOT EXISTS databricks_pdf_documentation (\n",
    "  id BIGINT GENERATED BY DEFAULT AS IDENTITY,\n",
    "  url STRING,\n",
    "  content STRING,\n",
    "  embedding ARRAY <FLOAT>\n",
    ") TBLPROPERTIES (delta.enableChangeDataFeed = true); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "263e0789-6f92-45ce-a49f-cf0e5fd8432f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Computing the chunk embeddings and saving them to our Delta Table\n",
    "\n",
    "The last step is to now compute an embedding for all our documentation chunks. Let's create an udf to compute the embeddings using the foundation model endpoint.\n",
    "\n",
    "*Note that this part would typically be setup as a production-grade job, running as soon as a new documentation page is updated. <br/> This could be setup as a Delta Live Table pipeline to incrementally consume updates.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6dd15c71-e9db-4e87-bc36-1227d33a20e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@pandas_udf(\"array<float>\")\n",
    "def get_embedding(contents: pd.Series) -> pd.Series:\n",
    "    import mlflow.deployments\n",
    "    deploy_client = mlflow.deployments.get_deploy_client(\"databricks\")\n",
    "    def get_embeddings(batch):\n",
    "        #Note: this will fail if an exception is thrown during embedding creation (add try/except if needed) \n",
    "        response = deploy_client.predict(endpoint=\"databricks-gte-large-en\", inputs={\"input\": batch})\n",
    "        return [e['embedding'] for e in response.data]\n",
    "\n",
    "    # Splitting the contents into batches of 150 items each, since the embedding model takes at most 150 inputs per request.\n",
    "    max_batch_size = 150\n",
    "    batches = [contents.iloc[i:i + max_batch_size] for i in range(0, len(contents), max_batch_size)]\n",
    "\n",
    "    # Process each batch and collect the results\n",
    "    all_embeddings = []\n",
    "    for batch in batches:\n",
    "        all_embeddings += get_embeddings(batch.tolist())\n",
    "\n",
    "    return pd.Series(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55066252-4c28-4129-b065-72f6e8f555fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(spark.readStream.table('pdf_raw')\n",
    "      .withColumn(\"content\", F.explode(read_as_chunk(\"content\")))\n",
    "      .filter(\"content not like '__PDF_PARSING_ERROR__%'\") #Drop PDF with parsing ERROR (could throw an error instead or properly flag that in a prod setup to avoid silent failures)\n",
    "      .withColumn(\"embedding\", get_embedding(\"content\"))\n",
    "      .selectExpr('path as url', 'content', 'embedding')\n",
    "  .writeStream\n",
    "    .trigger(availableNow=True)\n",
    "    .option(\"checkpointLocation\", f'dbfs:{volume_folder}/checkpoints/pdf_chunk')\n",
    "    .table('databricks_pdf_documentation').awaitTermination())\n",
    "\n",
    "#Let's also add our documentation web page from the simple demo (make sure you run the quickstart demo first)\n",
    "if spark.catalog.tableExists(f'{catalog}.{db}.databricks_documentation'):\n",
    "  (spark.readStream.option(\"skipChangeCommits\", \"true\").table('databricks_documentation') #skip changes for more stable demo\n",
    "      .withColumn('embedding', get_embedding(\"content\"))\n",
    "      .select('url', 'content', 'embedding')\n",
    "  .writeStream\n",
    "    .trigger(availableNow=True)\n",
    "    .option(\"checkpointLocation\", f'dbfs:{volume_folder}/checkpoints/docs_chunks')\n",
    "    .table('databricks_pdf_documentation').awaitTermination())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9078a747-4144-4a5e-8e4b-25bb8855fc5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM databricks_pdf_documentation WHERE url like '%.pdf' limit 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "0a2d55f2-589c-4a64-b038-d89cbb16df3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Our dataset is now ready! Let's create our Self-Managed Vector Search Index.\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/rag-pdf-self-managed-3.png?raw=true\" style=\"float: right; width: 600px; margin-left: 10px\">\n",
    "\n",
    "Our dataset is now ready. We chunked the documentation pages into small sections, computed the embeddings and saved it as a Delta Lake table.\n",
    "\n",
    "Next, we'll configure Databricks Vector Search to ingest data from this table.\n",
    "\n",
    "Vector search index uses a Vector search endpoint to serve the embeddings (you can think about it as your Vector Search API endpoint). <br/>\n",
    "Multiple Indexes can use the same endpoint. Let's start by creating one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6df0b558-c8da-428a-9524-0b989289680f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Creating the Vector Search endpoint"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.vector_search.client import VectorSearchClient\n",
    "vsc = VectorSearchClient()\n",
    "\n",
    "if not endpoint_exists(vsc, VECTOR_SEARCH_ENDPOINT_NAME):\n",
    "    vsc.create_endpoint(name=VECTOR_SEARCH_ENDPOINT_NAME, endpoint_type=\"STANDARD\")\n",
    "\n",
    "wait_for_vs_endpoint_to_be_ready(vsc, VECTOR_SEARCH_ENDPOINT_NAME)\n",
    "print(f\"Endpoint named {VECTOR_SEARCH_ENDPOINT_NAME} is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc1ce2e1-12cf-4c4f-ba33-14723ba1296f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "You can view your endpoint on the [Vector Search Endpoints UI](#/setting/clusters/vector-search). Click on the endpoint name to see all indexes that are served by the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "501c5a50-c168-4b13-af9d-b658f41c1dbc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create the Self-managed vector search using our endpoint"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "import databricks.sdk.service.catalog as c\n",
    "\n",
    "#The table we'd like to index\n",
    "source_table_fullname = f\"{catalog}.{db}.databricks_pdf_documentation\"\n",
    "# Where we want to store our index\n",
    "vs_index_fullname = f\"{catalog}.{db}.databricks_pdf_documentation_self_managed_vs_index\"\n",
    "\n",
    "if not index_exists(vsc, VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname):\n",
    "  print(f\"Creating index {vs_index_fullname} on endpoint {VECTOR_SEARCH_ENDPOINT_NAME}...\")\n",
    "  try:\n",
    "    vsc.create_delta_sync_index(\n",
    "      endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME,\n",
    "      index_name=vs_index_fullname,\n",
    "      source_table_name=source_table_fullname,\n",
    "      pipeline_type=\"TRIGGERED\", #Sync needs to be manually triggered\n",
    "      primary_key=\"id\",\n",
    "      embedding_dimension=1024, #Match your model embedding size (gte)\n",
    "      embedding_vector_column=\"embedding\"\n",
    "    )\n",
    "  except Exception as e:\n",
    "    display_quota_error(e, VECTOR_SEARCH_ENDPOINT_NAME)\n",
    "    raise e\n",
    "  #Let's wait for the index to be ready and all our embeddings to be created and indexed\n",
    "  wait_for_index_to_be_ready(vsc, VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname)\n",
    "else:\n",
    "  #Trigger a sync to update our vs content with the new data saved in the table\n",
    "  wait_for_index_to_be_ready(vsc, VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname)\n",
    "  vsc.get_index(VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname).sync()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9400370b-849b-48cb-b4b8-90891c31b649",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Searching for similar content\n",
    "\n",
    "That's all we have to do. Databricks will automatically capture and synchronize new entries in your Delta Lake Table.\n",
    "\n",
    "Note that depending on your dataset size and model size, index creation can take a few seconds to start and index your embeddings.\n",
    "\n",
    "Let's give it a try and search for similar content.\n",
    "\n",
    "*Note: `similarity_search` also supports a filters parameter. This is useful to add a security layer to your RAG system: you can filter out some sensitive content based on who is doing the call (for example filter on a specific department based on the user preference).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3571abb8-b076-4c67-9ba5-5dd5cc2ebc6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "question = \"How can I track billing usage on my workspaces?\"\n",
    "\n",
    "response = deploy_client.predict(endpoint=\"databricks-gte-large-en\", inputs={\"input\": [question]})\n",
    "embeddings = [e['embedding'] for e in response.data]\n",
    "\n",
    "results = vsc.get_index(VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname).similarity_search(\n",
    "  query_vector=embeddings[0],\n",
    "  columns=[\"url\", \"content\"],\n",
    "  num_results=1)\n",
    "docs = results.get('result', {}).get('data_array', [])\n",
    "pprint(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02880b79-ad34-4994-b593-524a43aa4e37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Next step: Deploy our chatbot model with RAG\n",
    "\n",
    "We've seen how Databricks Lakehouse AI makes it easy to ingest and prepare your documents, and deploy a Self Managed Vector Search index on top of it with just a few lines of code and configuration.\n",
    "\n",
    "This simplifies and accelerates your data projects so that you can focus on the next step: creating your realtime chatbot endpoint with well-crafted prompt augmentation.\n",
    "\n",
    "Open the [02-Advanced-Chatbot-Chain]($./02-Advanced-Chatbot-Chain) notebook to create and deploy a chatbot endpoint."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "01-PDF-Advanced-Data-Preparation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
