{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "a3315d08-a97d-4438-b2b3-89167514e7d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Monitoring your chatbot behavior over time!\n",
    "\n",
    "<!-- <img src=\"https://docs.databricks.com/en/_images/online-monitoring-dashboard.gif\" style=\"float:right\" width=\"750px\" /> -->\n",
    "\n",
    "In the previous notebook, we have investigated the evaluation input to create an evaluation dataset and benchmark newer version against it.\n",
    "\n",
    "Because Databricks also tracks all your customer discussions, we can also deploy online monitoring, directly reviewing where your model doesn't perform well and trigger alerts and ask for your expert to improve your dataset!\n",
    "\n",
    "This is also a good opportunity to either improve your documentation or adjust your prompt, and ultimately add the correct answer to your evaluation dataset!\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection or disable tracker during installation. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=data-science&org_id=341332174749405&notebook=%2F03-advanced-app%2F04-Online-Evaluation&demo_name=llm-rag-chatbot&event=VIEW&path=%2F_dbdemos%2Fdata-science%2Fllm-rag-chatbot%2F03-advanced-app%2F04-Online-Evaluation&version=1\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c621e28-6866-4c47-88d2-64b0480c8205",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --quiet -U databricks-sdk==0.49.0 databricks-agents\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a311a60d-b47a-4e2d-9c1d-56c2a48d2edc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ../_resources/00-init-advanced $reset_all_data=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "268c86cd-aacd-435a-a8b7-d8b4aec52cea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.removeAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "127117ce-2301-4bf6-81b3-20a151ecdc84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(name=\"endpoint_name\", defaultValue=f\"agents_{catalog}-{db}-rag_demo_advanced\", label=\"1. Agent's Model serving endpoint name\")\n",
    "\n",
    "dbutils.widgets.text(name=\"topics\", defaultValue='''\"spark\", \"AI\", \"DBSQL\",\"other\"''', label=\"\"\"3. List of topics in which to categorize the input requests. Format must be comma-separated strings in double quotes. We recommend having an \"other\" category as a catch-all.\"\"\")\n",
    "\n",
    "dbutils.widgets.text(name=\"sample_rate\", defaultValue=\"0.3\", label=\"2. Sampling rate between 0.0 and 1.0: on what % of requests should the LLM judge quality analysis run?\")\n",
    "\n",
    "local_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "local_path = '/'.join(local_path.split('/')[:-1])\n",
    "dbutils.widgets.text(name=\"workspace_folder\", defaultValue=local_path, label=\"4. Folder to create dashboard.\")\n",
    "\n",
    "endpoint_name = dbutils.widgets.get(\"endpoint_name\")\n",
    "from databricks.sdk import WorkspaceClient\n",
    "wc = WorkspaceClient()\n",
    "ep = wc.serving_endpoints.get(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "153c9d51-8579-4d82-81b7-914b9cc86322",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Start by protecting your endpoint: setting up AI Guardrails\n",
    "\n",
    "### Set invalid keywords\n",
    "\n",
    "You can investigate the inference table to see whether the endpoint is being used for inappropriate topics. From the inference table, it looks like a user is talking about SuperSecretProject! For this example, you can assume that topic is not in the scope of use for this chat endpoint.\n",
    "\n",
    "### Set up PII detection\n",
    "\n",
    "Now, the endpoint blocks messages referencing SuperSecretProject. You can also make sure the endpoint doesn't accept requests with or respond with messages containing any PII.\n",
    "\n",
    "The following updates the guardrails configuration for pii:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7a6b489-a83a-4e67-90e2-91f5179ce4fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk.service.serving import AiGatewayGuardrails, AiGatewayRateLimit,AiGatewayRateLimitRenewalPeriod, AiGatewayUsageTrackingConfig\n",
    "\n",
    "guardrails = AiGatewayGuardrails.from_dict({\n",
    "    \"input\": {\n",
    "        \"pii\": {\"behavior\": \"BLOCK\"},\n",
    "        \"invalid_keywords\": [\"SuperSecretProject\"]\n",
    "    },\n",
    "    \"output\": {\n",
    "        \"pii\": {\"behavior\": \"BLOCK\"}\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6c2d64d-3dbb-40d6-be7a-fb9b31e74ca9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Add rate limits\n",
    "\n",
    "Say you are investigating the inference tables further and you see some steep spikes in usage suggesting a higher-than-expected volume of queries. Extremely high usage could be costly if not monitored and limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4292687-d4b5-4604-a818-56ef8439a67d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rate_limits = AiGatewayRateLimit(calls=100,\n",
    "                                renewal_period=AiGatewayRateLimitRenewalPeriod.MINUTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "a5d2e2bb-86b5-405a-bcc8-7a10c75cd51c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Let's enable the AI gateway:\n",
    "\n",
    "<div style=\"background-color: #d4f8d4; border-radius: 15px; padding: 20px; text-align: center;\">\n",
    "        Note: AI gateway might not be enable in all workspaces, please reach out your account team if needed!\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8829f0a-b441-4ab9-8f50-89a3e1bc7d3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "wc.serving_endpoints.put_ai_gateway(endpoint_name, \n",
    "                                    guardrails=guardrails, \n",
    "                                    rate_limits=[rate_limits],\n",
    "                                    usage_tracking_config=AiGatewayUsageTrackingConfig(enabled=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24e001ae-4bda-44d2-a15e-206077cbd4f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Let's try our endpoint\n",
    "\n",
    "Say you are investigating the inference tables further and you see some steep spikes in usage suggesting a higher-than-expected volume of queries. Extremely high usage could be costly if not monitored and limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5615ed6e-cfb2-4fb9-bf64-a369da40357c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.deployments import get_deploy_client\n",
    "deploy_client = get_deploy_client(\"databricks\")\n",
    "\n",
    "for r in spark.table(\"eval_set_databricks_documentation\").limit(10).collect():\n",
    "    print(f\"Simulating traffic: {r['request']}\")\n",
    "    df = pd.DataFrame({'messages': [[{'content': r['request'], 'role': 'user'}]]})\n",
    "    result = deploy_client.predict(endpoint=\"agents_main__build-dbdemos_rag_chatbot-rag_demo_advanced\", inputs={\"dataframe_split\": df.to_dict(orient='split')})\n",
    "    print(f\"Model answer : {result['predictions'][0]['choices'][0]['message']}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12136ecd-2a17-448b-9484-1089ba1ad77d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Building custom Dashboard to track your online model\n",
    "\n",
    "Now that our model is in production through our AI gateway, we can add custom dashboard to track its behavior.\n",
    "\n",
    "Here is a script example that you can run to build and refresh an evaluation dashboard.\n",
    "\n",
    "These next cells are inspired from Databricks Documentation: https://docs.databricks.com/en/generative-ai/agent-evaluation/evaluating-production-traffic.html\n",
    "\n",
    "**We strongly recommend checking the full documentation for more details and up to date dashboard!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "423f5497-5de9-4755-a66d-c207bde0ae6c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "helper function to prepare our dashboard"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import List\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service import workspace\n",
    "from IPython.display import display_markdown\n",
    "from mlflow.utils import databricks_utils as du\n",
    "\n",
    "wc = WorkspaceClient()\n",
    "\n",
    "def get_payload_table_name(endpoint_info) -> str:\n",
    "  catalog_name = endpoint_info.config.auto_capture_config.catalog_name\n",
    "  schema_name = endpoint_info.config.auto_capture_config.schema_name\n",
    "  payload_table_name = endpoint_info.config.auto_capture_config.state.payload_table.name\n",
    "  return f\"{catalog_name}.{schema_name}.{payload_table_name}\"\n",
    "\n",
    "def get_model_name(endpoint_info) -> str:\n",
    "  served_models = [\n",
    "    model for model in endpoint_info.config.served_models if not model.model_name.endswith(\".feedback\")\n",
    "  ]\n",
    "  return served_models[0].model_name\n",
    "\n",
    "# Helper function for display Delta Table URLs\n",
    "def get_table_url(table_fqdn: str) -> str:\n",
    "    table_fqdn = table_fqdn.replace(\"`\", \"\")\n",
    "    split = table_fqdn.split(\".\")\n",
    "    browser_url = du.get_browser_hostname()\n",
    "    url = f\"https://{browser_url}/explore/data/{split[0]}/{split[1]}/{split[2]}\"\n",
    "    return url\n",
    "\n",
    "# Helper function to split the evaluation dataframe by hour\n",
    "def split_df_by_hour(df: pd.DataFrame, max_samples_per_hours: int) -> List[pd.DataFrame]:\n",
    "    df['hour'] = pd.to_datetime(df[\"timestamp\"]).dt.floor('H')\n",
    "    dfs_by_hour = [\n",
    "        group.sample(min(len(group), max_samples_per_hours), replace=False)\n",
    "        for _, group in df.groupby('hour')\n",
    "    ]\n",
    "    return dfs_by_hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc3547d0-5730-49b9-bcdb-52b6737f2554",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Read parameters"
    }
   },
   "outputs": [],
   "source": [
    "sample_rate = float(dbutils.widgets.get(\"sample_rate\"))\n",
    "# Verify that sample_rate is a number in [0,1]\n",
    "assert(0.0 <= sample_rate and sample_rate <= 1.0), 'Please specify a sample rate between 0.0 and 1.0'\n",
    "\n",
    "sample_rate_display = f\"{sample_rate:0.0%}\"\n",
    "\n",
    "workspace_folder = dbutils.widgets.get(\"workspace_folder\").replace(\"/Workspace\", \"\")\n",
    "if workspace_folder is None or workspace_folder == \"\":\n",
    "  username = spark.sql(\"select current_user() as username\").collect()[0][\"username\"]\n",
    "  workspace_folder=f\"/Users/{username}\"\n",
    "\n",
    "folder_info = wc.workspace.get_status(workspace_folder)\n",
    "assert (folder_info is not None and folder_info.object_type == workspace.ObjectType.DIRECTORY), f\"Please specify a valid workspace folder. The specified folder {workspace_folder} is invalid.\"\n",
    "\n",
    "topics = dbutils.widgets.get(\"topics\")\n",
    "\n",
    "# Print debugging information\n",
    "display_markdown(\"## Monitoring notebook configuration\", raw=True)\n",
    "display_markdown(f\"- **Agent Model Serving endpoint name:** `{endpoint_name}`\", raw=True)\n",
    "display_markdown(f\"- **% of requests that will be run through LLM judge quality analysis:** `{sample_rate_display}`\", raw=True)\n",
    "display_markdown(f\"- **Storing output artifacts in:** `{workspace_folder}`\", raw=True)\n",
    "display_markdown(f\"- **Topics to detect:** `{topics}`\", raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1fb18a8-a50c-4052-9b34-f4e57d395aca",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set up table-name variables"
    }
   },
   "outputs": [],
   "source": [
    "def escape_table_name(table_name: str) -> str:\n",
    "  return \".\".join(list(map(lambda x: f\"`{x}`\", table_name.split(\".\"))))\n",
    "\n",
    "# Deployed agents create multiple inference tables that can be used for further processing such as evaluation. See the documentation:\n",
    "# AWS documentation: https://docs.databricks.com/en/generative-ai/deploy-agent.html#agent-enhanced-inference-tables\n",
    "# Azure documentation: https://learn.microsoft.com/en-us/azure/databricks/generative-ai/deploy-agent#agent-enhanced-inference-tables\n",
    "endpoint_info = wc.serving_endpoints.get(endpoint_name)\n",
    "inference_table_name = get_payload_table_name(endpoint_info)\n",
    "fully_qualified_uc_model_name = get_model_name(endpoint_info)\n",
    "\n",
    "requests_log_table_name = f\"{inference_table_name}_request_logs\"\n",
    "eval_requests_log_table_name = escape_table_name(f\"{requests_log_table_name}_eval\")\n",
    "assessment_log_table_name = escape_table_name(f\"{inference_table_name}_assessment_logs\")\n",
    "\n",
    "eval_requests_log_checkpoint = f\"{requests_log_table_name}_eval_checkpoint\"\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {escape_table_name(eval_requests_log_checkpoint)}\")\n",
    "eval_requests_log_checkpoint_path = f\"/Volumes/{eval_requests_log_checkpoint.replace('.', '/')}\"\n",
    "\n",
    "# Print debugging information\n",
    "display_markdown(\"## Input tables\", raw=True)\n",
    "display_markdown(\n",
    "  f\"- **Inference table:** [{inference_table_name}]({get_table_url(inference_table_name)})\",\n",
    "  raw=True\n",
    ")\n",
    "display_markdown(\n",
    "  f\"- **Request logs table:** [{requests_log_table_name}]({get_table_url(requests_log_table_name)})\",\n",
    "  raw=True\n",
    ")\n",
    "display_markdown(\n",
    "  f'- **Human feedback logs table:** [{assessment_log_table_name.replace(\"`\", \"\")}]({get_table_url(assessment_log_table_name.replace(\"`\", \"\"))})',\n",
    "  raw=True\n",
    ")\n",
    "display_markdown(\"## Output tables/volumes\", raw=True)\n",
    "display_markdown(\n",
    "  f'- **LLM judge results table:** [{eval_requests_log_table_name.replace(\"`\", \"\")}]({get_table_url(eval_requests_log_table_name.replace(\"`\", \"\"))})',\n",
    "  raw=True\n",
    ")\n",
    "display_markdown(f\"- **Streaming checkpoints volume:** `{eval_requests_log_checkpoint_path}`\", raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10b35ad8-64ac-4f88-b196-63cb27f7341f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Initialize mlflow experiment"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow_client = mlflow.MlflowClient()\n",
    "\n",
    "# Create a single experiment to store all monitoring runs for this endpoint\n",
    "experiment_name = f\"{workspace_folder}/{inference_table_name}_experiment\"\n",
    "experiment = mlflow_client.get_experiment_by_name(experiment_name)\n",
    "experiment_id = mlflow_client.create_experiment(experiment_name) if experiment is None else experiment.experiment_id\n",
    "\n",
    "mlflow.set_experiment(experiment_name=experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1cf0c669-0f00-4a84-8be4-de1673aaf63b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Update the table with unprocessed requests"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Streams any unprocessed rows from the requests log table into the evaluation requests log table.\n",
    "# Unprocessed requests have an empty run_id.\n",
    "# Processed requests have one of the following values: \"skipped\", \"to_process\", or a valid run_id.\n",
    "(\n",
    "  spark.readStream.format(\"delta\")\n",
    "  .table(escape_table_name(requests_log_table_name))\n",
    "  .withColumn(\"run_id\", F.lit(None).cast(\"string\"))\n",
    "  .withColumn(\n",
    "    \"retrieval/llm_judged/chunk_relevance/precision\", F.lit(None).cast(\"double\")\n",
    "  )\n",
    "  .writeStream.option(\"checkpointLocation\", eval_requests_log_checkpoint_path)\n",
    "  .option(\"mergeSchema\", \"true\")\n",
    "  .format(\"delta\")\n",
    "  .outputMode(\"append\")\n",
    "  .trigger(availableNow=True)\n",
    "  .toTable(eval_requests_log_table_name)\n",
    "  .awaitTermination()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d642b8c-a15e-4730-83c8-2d6a48b86083",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Mark rows for processing, mark the rest as \"skipped\""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "          WITH sampled_requests AS (\n",
    "            -- Sample unprocessed requests\n",
    "            SELECT *\n",
    "            FROM (\n",
    "              SELECT databricks_request_id\n",
    "              FROM {eval_requests_log_table_name}\n",
    "              WHERE \n",
    "                run_id IS NULL\n",
    "                AND `timestamp` >= CURRENT_TIMESTAMP() - INTERVAL 30 DAY\n",
    "            ) TABLESAMPLE ({sample_rate*100} PERCENT)\n",
    "          ), requests_with_user_feedback AS (\n",
    "            -- Get unprocessed requests with user feedback\n",
    "            SELECT assessments.request_id\n",
    "            FROM {assessment_log_table_name} assessments\n",
    "            INNER JOIN {eval_requests_log_table_name} requests\n",
    "            ON assessments.request_id = requests.databricks_request_id\n",
    "            WHERE \n",
    "              requests.run_id is NULL\n",
    "              AND assessments.`timestamp` >= CURRENT_TIMESTAMP() - INTERVAL 30 DAY\n",
    "          )\n",
    "\n",
    "          -- Mark the selected rows as `to_process`\n",
    "          UPDATE {eval_requests_log_table_name} \n",
    "          SET run_id=\"to_process\"\n",
    "          WHERE databricks_request_id IN (\n",
    "            SELECT * FROM sampled_requests\n",
    "            UNION \n",
    "            SELECT * FROM requests_with_user_feedback\n",
    "          )\n",
    "          \"\"\")\n",
    "\n",
    "###############\n",
    "# CONFIG: Add custom logic here to select more rows. Update the run_id of selected rows to the value \"to_process\".\n",
    "###############\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "          UPDATE {eval_requests_log_table_name}\n",
    "          SET run_id=\"skipped\"\n",
    "          WHERE run_id IS NULL\n",
    "          \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7969c56-4338-4c77-a5d4-56ef6bf75d76",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Run evaluate on unprocessed rows"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.rag_eval import env_vars\n",
    "\n",
    "eval_df = spark.sql(f\"\"\"\n",
    "                    SELECT \n",
    "                      `timestamp`,\n",
    "                      databricks_request_id as request_id, \n",
    "                      from_json(request_raw, 'STRUCT<messages ARRAY<STRUCT<role STRING, content STRING>>>') AS request,\n",
    "                      trace\n",
    "                    FROM {eval_requests_log_table_name} \n",
    "                    WHERE run_id=\"to_process\"                  \n",
    "                    \"\"\")\n",
    "\n",
    "eval_pdf = eval_df.toPandas().drop_duplicates(subset=[\"request_id\"])\n",
    "\n",
    "if eval_pdf.empty:\n",
    "  print(\"[Warning] No new rows to process.\")\n",
    "else:\n",
    "  with mlflow.start_run() as run:\n",
    "    ###############\n",
    "    # CONFIG: Adjust mlflow.evaluate(...) to change which Databricks LLM judges are run. By default, judges that do not require ground truths\n",
    "    # are run, including groundedness, safety, chunk relevance, and relevance to query. For more details, see the documentation:\n",
    "    # AWS documentation: https://docs.databricks.com/en/generative-ai/agent-evaluation/advanced-agent-eval.html#evaluate-agents-using-a-subset-of-llm-judges\n",
    "    # Azure documentation: https://learn.microsoft.com/en-us/azure/databricks/generative-ai/agent-evaluation/advanced-agent-eval#evaluate-agents-using-a-subset-of-llm-judges\n",
    "    ###############\n",
    "    for eval_pdf_batch in split_df_by_hour(eval_pdf, max_samples_per_hours=env_vars.RAG_EVAL_MAX_INPUT_ROWS.get()):\n",
    "      eval_results = mlflow.evaluate(data=eval_pdf_batch, model_type=\"databricks-agent\")\n",
    "\n",
    "      results_df = (\n",
    "        spark\n",
    "        .createDataFrame(eval_results.tables['eval_results'])\n",
    "        .withColumn(\"databricks_request_id\", F.col(\"request_id\"))\n",
    "        .withColumn(\"run_id\", F.lit(run.info.run_id).cast(\"string\"))\n",
    "        .withColumn(\"experiment_id\", F.lit(experiment_id).cast(\"string\"))\n",
    "        .withColumn(\"topic\", F.lit(None).cast(\"string\"))\n",
    "        .drop(\"request_id\")\n",
    "        .drop(\"request\")\n",
    "        .drop(\"response\")\n",
    "        .drop(\"trace\")\n",
    "      )\n",
    "\n",
    "      results_df.createOrReplaceTempView(\"updates\")\n",
    "\n",
    "      spark.sql(f\"\"\"\n",
    "                merge with schema evolution into {eval_requests_log_table_name} evals \n",
    "                using updates ON evals.databricks_request_id=updates.databricks_request_id \n",
    "                WHEN MATCHED THEN UPDATE SET *\n",
    "                \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "598c2c46-9b91-4652-bf11-7a789075cc82",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Perform topic detection"
    }
   },
   "outputs": [],
   "source": [
    "# Perform topic detection using the `ai_classify` function. For more details, see the documentation:\n",
    "# AWS documentation: https://docs.databricks.com/en/sql/language-manual/functions/ai_classify.html\n",
    "# Azure documentation: https://learn.microsoft.com/en-us/azure/databricks/sql/language-manual/functions/ai_classify\n",
    "if not eval_pdf.empty:   \n",
    "  if not len(topics.strip()) or topics == \"\\\"other\\\"\":\n",
    "    spark.sql(f\"\"\"\n",
    "              merge with schema evolution into {eval_requests_log_table_name} evals \n",
    "              using updates ON evals.databricks_request_id=updates.databricks_request_id \n",
    "              WHEN MATCHED THEN UPDATE SET topic=\"other\"\n",
    "              \"\"\")\n",
    "  else:          \n",
    "    spark.sql(f\"\"\"\n",
    "              merge with schema evolution into {eval_requests_log_table_name} evals \n",
    "              using updates ON evals.databricks_request_id=updates.databricks_request_id \n",
    "              WHEN MATCHED THEN UPDATE SET topic=ai_classify(request, ARRAY({topics}))\n",
    "              \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d2f4c25-3942-42df-81fd-5365ae3293e2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load the dashboard template"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "dashboard_template_url = 'https://raw.githubusercontent.com/databricks/genai-cookbook/main/rag_app_sample_code/resources/agent_quality_online_monitoring_dashboard_template.json'\n",
    "response = requests.get(dashboard_template_url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "  dashboard_template = str(response.text)\n",
    "else:\n",
    "  raise Exception(\"Failed to get the dashboard template. Please try again or download the template directly from the URL.\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe337998-8f6a-41da-8bd6-c93c3a1b1a98",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create or get the dashboard"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk import errors\n",
    "\n",
    "wc = WorkspaceClient()\n",
    "object_info = None\n",
    "\n",
    "num_evaluated_requests = spark.sql(f\"select count(*) as num_rows from {eval_requests_log_table_name}\").collect()[0].num_rows\n",
    "if not num_evaluated_requests:\n",
    "  print(\"There are no evaluated requests! Skipping dashboard creation.\")\n",
    "else:\n",
    "  dashboard_name = f\"Monitoring dashboard for {fully_qualified_uc_model_name}\"\n",
    "  try:\n",
    "    dashboard_content = (\n",
    "      dashboard_template\n",
    "        .replace(\"{{inference_table_name}}\", inference_table_name)\n",
    "        .replace(\"{{eval_requests_log_table_name}}\", eval_requests_log_table_name)\n",
    "        .replace(\"{{assessment_log_table_name}}\", assessment_log_table_name)\n",
    "        .replace(\"{{fully_qualified_uc_model_name}}\", fully_qualified_uc_model_name)\n",
    "        .replace(\"{{sample_rate}}\", sample_rate_display)\n",
    "    )\n",
    "    object_info = wc.workspace.get_status(path=f\"{workspace_folder}/{dashboard_name}.lvdash.json\")\n",
    "    dashboard_id = object_info.resource_id\n",
    "  except errors.platform.ResourceDoesNotExist as e:\n",
    "    dashboard_info = wc.lakeview.create(display_name=dashboard_name, serialized_dashboard=dashboard_content, parent_path=workspace_folder)\n",
    "    dashboard_id = dashboard_info.dashboard_id\n",
    "  \n",
    "  displayHTML(f\"\"\"<a href=\"sql/dashboardsv3/{dashboard_id}\">Dashboard</a>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dcc91ab9-e2ef-49e9-ad17-2ce586ff5585",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "That's it!\n",
    "\n",
    "Our model is now deployed in production, and you can periodically run these last cells to refresh your monitoring dashboard and track its behavior in production."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04-Online-Evaluation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
