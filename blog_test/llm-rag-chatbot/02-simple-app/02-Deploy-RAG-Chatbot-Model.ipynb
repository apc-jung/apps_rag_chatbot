{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "18327f54-2a24-4d15-b30c-bf7db2f1a962",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2/ Creating the chatbot with Retrieval Augmented Generation (RAG) and DBRX Instruct\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/llm-rag-managed-flow-2.png?raw=true\" style=\"float: right; margin-left: 10px\"  width=\"900px;\">\n",
    "\n",
    "Our Vector Search Index is now ready!\n",
    "\n",
    "Let's now create and deploy a new Model Serving Endpoint to perform RAG.\n",
    "\n",
    "The flow will be the following:\n",
    "\n",
    "- A user asks a question\n",
    "- The question is sent to our serverless Chatbot RAG endpoint\n",
    "- The endpoint compute the embeddings and searches for docs similar to the question, leveraging the Vector Search Index\n",
    "- The endpoint creates a prompt enriched with the doc\n",
    "- The prompt is sent to the DBRX Instruct Foundation Model Serving Endpoint\n",
    "- We display the output to our users!\n",
    "\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection or disable tracker during installation. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=data-science&org_id=341332174749405&notebook=%2F02-simple-app%2F02-Deploy-RAG-Chatbot-Model&demo_name=llm-rag-chatbot&event=VIEW&path=%2F_dbdemos%2Fdata-science%2Fllm-rag-chatbot%2F02-simple-app%2F02-Deploy-RAG-Chatbot-Model&version=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a2b1972-5280-4d0a-8c50-a3255f370d22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "*Note: RAG performs document searches using Databricks Vector Search. In this notebook, we assume that the search index is ready for use. Make sure you run the previous [01-Data-Preparation-and-Index]($./01-Data-Preparation-and-Index [DO NOT EDIT]) notebook.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97911368-b208-447b-b505-a669605a7f2c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install the required libraries"
    }
   },
   "outputs": [],
   "source": [
    "%pip install --quiet -U databricks-sdk==0.49.0 \"databricks-langchain>=0.4.0\" databricks-agents mlflow[databricks] langchain==0.3.25 langchain_core==0.3.59 databricks-vectorsearch==0.55 pydantic==2.10.1\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efbc6c2c-9d1a-4a54-92b5-875b3e290ce3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ../_resources/00-init $reset_all_data=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "5073aa36-a443-4b51-a25e-2be00e152495",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Building our Chain\n",
    "\n",
    "In this example, we'll assume you already have a basic understanding of langchain. Check our [previous notebook]($../00-first-step/01-First-Step-RAG-On-Databricks) to take it one step at a time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "acc34ae3-6f86-4d2e-91a7-02f8239218e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rag_chain_config = {\n",
    "    \"databricks_resources\": {\n",
    "        \"llm_endpoint_name\": \"databricks-meta-llama-3-3-70b-instruct\",\n",
    "        \"vector_search_endpoint_name\": VECTOR_SEARCH_ENDPOINT_NAME,\n",
    "    },\n",
    "    \"input_example\": {\n",
    "        \"messages\": [{\"content\": \"What is Apache Spark?\", \"role\": \"user\"}]\n",
    "    },\n",
    "    \"llm_config\": {\n",
    "        \"llm_parameters\": {\"max_tokens\": 1500, \"temperature\": 0.01},\n",
    "        \"llm_prompt_template\": \"You are a trusted AI assistant that helps answer questions based only on the provided information. If you do not know the answer to a question, you truthfully say you do not know. Here is the history of the current conversation you are having with your user: {chat_history}. And here is some context which may or may not help you answer the following question: {context}.  Answer directly, do not repeat the question, do not start with something like: the answer to the question, do not add AI in front of your answer, do not say: here is the answer, do not mention the context or the question. Based on this context, answer this question: {question}\",\n",
    "        \"llm_prompt_template_variables\": [\"context\", \"chat_history\", \"question\"],\n",
    "    },\n",
    "    \"retriever_config\": {\n",
    "        \"chunk_template\": \"Passage: {chunk_text}\\n\",\n",
    "        \"data_pipeline_tag\": \"poc\",\n",
    "        \"parameters\": {\"k\": 5, \"query_type\": \"ann\"},\n",
    "        \"schema\": {\"chunk_text\": \"content\", \"document_uri\": \"url\", \"primary_key\": \"id\"},\n",
    "        \"vector_search_index\": f\"{catalog}.{db}.databricks_documentation_vs_index\",\n",
    "    },\n",
    "}\n",
    "try:\n",
    "    with open('rag_chain_config.yaml', 'w') as f:\n",
    "        yaml.dump(rag_chain_config, f)\n",
    "except:\n",
    "    print('pass to work on build job')\n",
    "model_config = mlflow.models.ModelConfig(development_config='rag_chain_config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfd88429-fd96-4ba0-acb4-97f76275a7e3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Write the chain to a companion file to avoid serialization issues"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile chain.py\n",
    "import os\n",
    "import mlflow\n",
    "from operator import itemgetter\n",
    "from databricks_langchain.chat_models import ChatDatabricks\n",
    "from databricks_langchain.vectorstores import DatabricksVectorSearch\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "## Enable MLflow Tracing\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "# Return the string contents of the most recent message from the user\n",
    "def extract_user_query_string(chat_messages_array):\n",
    "    return chat_messages_array[-1][\"content\"]\n",
    "\n",
    "def extract_previous_messages(chat_messages_array):\n",
    "    messages = \"\\n\"\n",
    "    for msg in chat_messages_array[:-1]:\n",
    "        messages += (msg[\"role\"] + \": \" + msg[\"content\"] + \"\\n\")\n",
    "    return messages\n",
    "\n",
    "def combine_all_messages_for_vector_search(chat_messages_array):\n",
    "    return extract_previous_messages(chat_messages_array) + extract_user_query_string(chat_messages_array)\n",
    "\n",
    "#Get the conf from the local conf file\n",
    "model_config = mlflow.models.ModelConfig(development_config='rag_chain_config.yaml')\n",
    "\n",
    "databricks_resources = model_config.get(\"databricks_resources\")\n",
    "retriever_config = model_config.get(\"retriever_config\")\n",
    "llm_config = model_config.get(\"llm_config\")\n",
    "\n",
    "vector_search_schema = retriever_config.get(\"schema\")\n",
    "\n",
    "# Turn the Vector Search index into a LangChain retriever\n",
    "vector_search_as_retriever = DatabricksVectorSearch(\n",
    "    endpoint=databricks_resources.get(\"vector_search_endpoint_name\"),\n",
    "    index_name=retriever_config.get(\"vector_search_index\"),\n",
    "    columns=[\n",
    "        vector_search_schema.get(\"primary_key\"),\n",
    "        vector_search_schema.get(\"chunk_text\"),\n",
    "        vector_search_schema.get(\"document_uri\"),\n",
    "    ],\n",
    ").as_retriever(search_kwargs=retriever_config.get(\"parameters\"))\n",
    "\n",
    "# Required to:\n",
    "# 1. Enable the RAG Studio Review App to properly display retrieved chunks\n",
    "# 2. Enable evaluation suite to measure the retriever\n",
    "mlflow.models.set_retriever_schema(\n",
    "    primary_key=vector_search_schema.get(\"primary_key\"),\n",
    "    text_column=vector_search_schema.get(\"chunk_text\"),\n",
    "    doc_uri=vector_search_schema.get(\"document_uri\")\n",
    ")\n",
    "\n",
    "# Method to format the docs returned by the retriever into the prompt\n",
    "def format_context(docs):\n",
    "    chunk_template = retriever_config.get(\"chunk_template\")\n",
    "    chunk_contents = [\n",
    "        chunk_template.format(\n",
    "            chunk_text=d.page_content,\n",
    "        )\n",
    "        for d in docs\n",
    "    ]\n",
    "    return \"\".join(chunk_contents)\n",
    "\n",
    "# Prompt Template for generation\n",
    "prompt = PromptTemplate(\n",
    "    template=llm_config.get(\"llm_prompt_template\"),\n",
    "    input_variables=llm_config.get(\"llm_prompt_template_variables\"),\n",
    ")\n",
    "\n",
    "# FM for generation\n",
    "model = ChatDatabricks(\n",
    "    endpoint=databricks_resources.get(\"llm_endpoint_name\"),\n",
    "    extra_params=llm_config.get(\"llm_parameters\"),\n",
    ")\n",
    "\n",
    "# RAG Chain\n",
    "chain = (\n",
    "    {\n",
    "        \"question\": itemgetter(\"messages\") | RunnableLambda(extract_user_query_string),\n",
    "        \"context\": itemgetter(\"messages\")\n",
    "        | RunnableLambda(combine_all_messages_for_vector_search)\n",
    "        | vector_search_as_retriever\n",
    "        | RunnableLambda(format_context),\n",
    "        \"chat_history\": itemgetter(\"messages\") | RunnableLambda(extract_previous_messages)\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Tell MLflow logging where to find your chain.\n",
    "mlflow.models.set_model(model=chain)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# chain.invoke(model_config.get(\"input_example\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af683ee5-f1c3-447f-9741-c5c952823732",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.models.resources import DatabricksVectorSearchIndex, DatabricksServingEndpoint\n",
    "\n",
    "# Log the model to MLflow\n",
    "with mlflow.start_run(run_name=\"dbdemos_rag_quickstart\"):\n",
    "    logged_chain_info = mlflow.langchain.log_model(\n",
    "        lc_model=os.path.join(os.getcwd(), 'chain.py'),  # Chain code file e.g., /path/to/the/chain.py \n",
    "        model_config='rag_chain_config.yaml',  # Chain configuration \n",
    "        artifact_path=\"chain\",  # Required by MLflow\n",
    "        input_example=model_config.get(\"input_example\"),  # Save the chain's input schema\n",
    "        resources=[\n",
    "            DatabricksVectorSearchIndex(index_name=model_config.get(\"retriever_config\").get(\"vector_search_index\")),\n",
    "            DatabricksServingEndpoint(endpoint_name=model_config.get(\"databricks_resources\").get(\"llm_endpoint_name\"))\n",
    "        ],\n",
    "        extra_pip_requirements=[\"databricks-connect\"]\n",
    "    )\n",
    "\n",
    "# Test the chain locally\n",
    "chain = mlflow.langchain.load_model(logged_chain_info.model_uri)\n",
    "chain.invoke(model_config.get(\"input_example\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45a02915-c338-436a-9cec-c0a2d579676a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Let's deploy our RAG application and open it for external expert users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9fd0e02b-6a64-4bec-84e6-bbe739974f1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks import agents\n",
    "MODEL_NAME = \"dbdemos_rag_demo\"\n",
    "MODEL_NAME_FQN = f\"{catalog}.{db}.{MODEL_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7514de8a-63f0-4d32-aa79-d0da1df4e7c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "instructions_to_reviewer = f\"\"\"### Instructions for Testing the our Databricks Documentation Chatbot assistant\n",
    "\n",
    "Your inputs are invaluable for the development team. By providing detailed feedback and corrections, you help us fix issues and improve the overall quality of the application. We rely on your expertise to identify any gaps or areas needing enhancement.\n",
    "\n",
    "1. **Variety of Questions**:\n",
    "   - Please try a wide range of questions that you anticipate the end users of the application will ask. This helps us ensure the application can handle the expected queries effectively.\n",
    "\n",
    "2. **Feedback on Answers**:\n",
    "   - After asking each question, use the feedback widgets provided to review the answer given by the application.\n",
    "   - If you think the answer is incorrect or could be improved, please use \"Edit Answer\" to correct it. Your corrections will enable our team to refine the application's accuracy.\n",
    "\n",
    "3. **Review of Returned Documents**:\n",
    "   - Carefully review each document that the system returns in response to your question.\n",
    "   - Use the thumbs up/down feature to indicate whether the document was relevant to the question asked. A thumbs up signifies relevance, while a thumbs down indicates the document was not useful.\n",
    "\n",
    "Thank you for your time and effort in testing our assistant. Your contributions are essential to delivering a high-quality product to our end users.\"\"\"\n",
    "\n",
    "# Register the chain to UC\n",
    "uc_registered_model_info = mlflow.register_model(model_uri=logged_chain_info.model_uri, name=MODEL_NAME_FQN)\n",
    "\n",
    "# Deploy to enable the Review APP and create an API endpoint\n",
    "deployment_info = agents.deploy(model_name=MODEL_NAME_FQN, model_version=uc_registered_model_info.version, scale_to_zero=True)\n",
    "\n",
    "# Add the user-facing instructions to the Review App\n",
    "agents.set_review_instructions(MODEL_NAME_FQN, instructions_to_reviewer)\n",
    "\n",
    "wait_for_model_serving_endpoint_to_be_ready(deployment_info.endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f26a27a2-054c-4779-8876-e715ca4964d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Grant stakeholders access to the Mosaic AI Agent Evaluation App\n",
    "\n",
    "Now, grant your stakeholders permissions to use the Review App. To simplify access, stakeholders do not require to have Databricks accounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb8c714f-20a3-4254-b0d8-11c2190510e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "user_list = [\"quentin.ambard@databricks.com\"]\n",
    "# Set the permissions.\n",
    "agents.set_permissions(model_name=MODEL_NAME_FQN, users=user_list, permission_level=agents.PermissionLevel.CAN_QUERY)\n",
    "\n",
    "print(f\"Share this URL with your stakeholders: {deployment_info.review_app_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb55ee11-690e-4820-858f-6777ad8682fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Find review app name\n",
    "\n",
    "If you lose this notebook's state and need to find the URL to your Review App, you can list the chatbot deployed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac07a731-860c-401b-95f5-968df75ca090",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for deployment in agents.list_deployments():\n",
    "  if deployment.model_name == MODEL_NAME_FQN:\n",
    "    print(f\"Review App URL: {deployment.review_app_url}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e64da957-c211-4163-8dca-670aac685856",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Congratulations! You have deployed your first GenAI RAG model!\n",
    "\n",
    "You're now ready to deploy the same logic for your internal knowledge base leveraging Lakehouse AI.\n",
    "\n",
    "We've seen how the Lakehouse AI is uniquely positioned to help you solve your GenAI challenge:\n",
    "\n",
    "- Simplify Data Ingestion and preparation with Databricks Engineering Capabilities\n",
    "- Accelerate Vector Search  deployment with fully managed indexes\n",
    "- Leverage Databricks DBRX Instruct foundation model endpoint\n",
    "- Deploy realtime model endpoint to perform RAG and provide Q&A capabilities\n",
    "\n",
    "Lakehouse AI is uniquely positioned to accelerate your GenAI deployment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24843134-b51f-402d-962a-35b591d475fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Next: Deploying our GenAI Assistant application to end users with Databricks Lakehouse Application\n",
    "\n",
    "We are now ready to build a front end application so that our users can ask questions to the chatbot. \n",
    "\n",
    "Open the [03-Deploy-Frontend-Lakehouse-App]($./03-Deploy-Frontend-Lakehouse-App) how to deploy your first Lakehouse Application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7205805f-596b-4d35-91cc-9f28bcb054f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Cleanup\n",
    "\n",
    "To free up resources, please delete uncomment and run the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d5da1e2-0183-42de-b731-283817f4954e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# /!\\ THIS WILL DROP YOUR DEMO SCHEMA ENTIRELY /!\\ \n",
    "# cleanup_demo(catalog, db, serving_endpoint_name, f\"{catalog}.{db}.databricks_documentation_vs_index\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "02-Deploy-RAG-Chatbot-Model",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
